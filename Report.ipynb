{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"C://Users/viren/Desktop/p1_navigation/Banana_Windows_x86_64/Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# while True:\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have loaded our Unity environment and tested a random agent to ensure that everything works as expected. Now begins the code that will train and use a DQN agent to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by importing all the remaining files we need to make this happen. Please note that the line **from dqn_agent import Agent** uses an unmodified version of the DQN code used in the Lunar Landing project done earlier in the nanodegree. This file (and its dependency *QNetwork* from the *model.py* file) are also imported at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from dqn_agent import Agent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in the Lunar Lander program, we will instantiate an agent. The primary difference here is that while the *state_size* was 8 there, it is 37 here. This is an intrinsic quality of the environment itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=37, action_size=4, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write the code that uses the DQN to train in the environment. The code you see below is nearly identical to the Lunar Lander project but adapted with the following changes:\n",
    "\n",
    "1. Changes in the way a Unity environment returns the state.\n",
    "2. Changes in the way a Unity environment performs an action.\n",
    "3. Stop when the score reaches 13, not 200, as in the Lunar Lander project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the Learning Algorithm**\n",
    "\n",
    "Deep Reinforcement Learning uses Non-linear Function Approximators to calculate the action-values based directly on observations from the environment. We represent this as a Deep Neural Network and use Deep Learning to find the optimal parameters for these function approximators. The Deep Q-Learning algorithm represents the optimal action-value function q* ​as a neural network (instead of a table).\n",
    "\n",
    "Unfortunately, Reinforcement Learning is notoriously unstable when neural networks are used to represent the action values. The Deep Q-Learning algorithm addresses these instabilities by using two key features:\n",
    "\n",
    "- Experience Replay: A Replay Pool keeps track of a rolling history of past data. Here, the behavior distribution is averaged over many of its previous states, smoothing out learning and avoiding oscillations. The advantage is that each step of the experience is potentially used in many weight updates.\n",
    "- Fixed Q-Targets: A Target Network is used to represent the old Q-Function, which computes every action's loss during training. The issue with using a single network is that at each step of training, the Q-Function values change, and the value estimates can quickly spiral out of control.\n",
    "\n",
    "• Experience Replay: When the agent interacts with the environment, the sequence of experience tuples can be highly correlated (because at every step, the current action affects the next state). The naive Q-learning algorithm that learns from each of these experience tuples in sequential order runs the risk of getting swayed by the effects of this correlation. By instead keeping track of a replay buffer and using experience replay to sample from the buffer at random (as opposed to being sequential), we can prevent action values from oscillating or diverging catastrophically.\n",
    "\n",
    "The replay buffer contains a collection of experience tuples (S, A, R, S'). The tuples are gradually added to the buffer as we are interacting with the environment.\n",
    "\n",
    "The act of sampling a small batch of tuples from the replay buffer to learn is known as experience replay. In addition to breaking harmful correlations, experience replay allows us to learn more from individual tuples multiple times, recall rare occurrences, and in general, make better use of our experience.\n",
    "\n",
    "In this way, Experience Replay reduces the Reinforcement Learning problem's value-learning portion to a Supervised Learning task. We can then use well established Supervised Learning algorithms to solve the problem. We can also improve upon this idea by prioritizing experience tuples that are rare or more important.\n",
    "\n",
    "• Fixed Q-Targets: Apart from the correlation between consecutive experience tuples that Experience Replay addresses, Q-Learning is also susceptible to another kind of correlation, caused because we are trying to update a guess with a guess. To avoid this, we can update the parameters w in the network q̂ to better approximate the action value corresponding to state S and action A with the following update rule:\n",
    "\n",
    "Δw = α . (R + γ max q̂ (S', a, w-) - q̂ (S, A, w)) ∇w q̂ (S, A, w)\n",
    "\n",
    "Where w- are the weights of a separate target network that are not changed during the learning step, and (S, A, R, S') is an experience tuple.\n",
    "\n",
    "R + γ max q̂ (S', a, w-) is the Temporal Difference target.\n",
    "\n",
    "Thus, the Deep Q-Learning algorithm uses two separate networks with identical architectures. The Target Q-Network's weights are updated less often than the primary Q-Network. Without fixed Q-Targets, we would encounter a harmful form of correlation, whereby we shift the network's parameters based on a continually moving target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Hyperparameters used**\n",
    "\n",
    "`BUFFER_SIZE = int(1e5)`\n",
    "\n",
    "The size of the Replay Buffer that holds tuples of experiences that can be selected multiple times.\n",
    "\n",
    "`BATCH_SIZE = 64`\n",
    "\n",
    "The number of entries from the Replay Buffer that are considered a part of each batch.\n",
    "\n",
    "`GAMMA = 0.99`\n",
    "\n",
    "The hyperparameter that prioritizes how much weightage is given to recently received rewards, as compared to previous rewards.\n",
    "\n",
    "`TAU = 1e-3`\n",
    "\n",
    "The architecture makes use of two networks- a fixed network and a target network. This hyperparameter is used for providing soft updates to the target network; i.e., instead of updating the values all at once, the process happens gradually, controlled with this hyperparameter.\n",
    "\n",
    "`LR = 5e-4`\n",
    "\n",
    "The learning rate with which the model learns.\n",
    "\n",
    "`UPDATE_EVERY = 4`\n",
    "\n",
    "How many steps to take before updating the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Model Architecture**\n",
    "\n",
    "Both networks, the regular and the target, have the same architecture. They consist of an input layer with 37 nodes (for the 37 dimension vector provided as the state), followed by two layers of 64 neurons each. ReLu activations are used to maintain the non-linearity in the models. The output layer consists of four nodes, representing each of the possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Training Process**\n",
    "\n",
    "The agent is trained for 2000 episodes, with 1000 timesteps per episode. Epsilon is used to balance the agent between exploration and exploitation. We start with a value of 1.0 and end with 0.01, decaying at a rate of 99.5% every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.74\n",
      "Episode 200\tAverage Score: 3.38\n",
      "Episode 300\tAverage Score: 6.42\n",
      "Episode 400\tAverage Score: 10.37\n",
      "Episode 500\tAverage Score: 12.73\n",
      "Episode 508\tAverage Score: 13.01\n",
      "Environment solved in 408 episodes!\tAverage Score: 13.01\n"
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        # Instead of saying: state = env.reset(), we must first create an env_info object as shown above\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        \n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            \n",
    "            # Instead of saying: next_state, reward, done, _ = env.step(action), we must do this step as shown above\n",
    "            # I had to add .astype(int) to the action variable to get the code to run on my PC\n",
    "            \n",
    "            env_info = env.step(action.astype(int))[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "        # '200' from the Lunar Lander project was changed to '13' to match the requirements\n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot of rewards per episode has been included to illustrate that the agent is capable of receiving an average reward of +13 over the last 100 episodes. From the previous code cell, we see that we have solved the environment in **408** episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now go ahead and close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As mentioned in the optional section of this project, we can use raw pixel data from the environment instead of the state vector of 37 dimensions currently returned.\n",
    "\n",
    "2. As mentioned in the suggestions section, we can implement a **Double DQN** or a **Dueling DQN**, with or without **Prioritized Experience Replay**, to get faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content in the following cells has been taken directly from the nanodegree lectures. I do not own this content but am noting it here to add to this submission's quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double DQN**\n",
    "\n",
    "Q-Learning is prone to an overestimation of the Q-Values. The arg_max operation in the TD target is necessary to find the best possible value we can get from the next state; we want to obtain the Q-value for the state S' and the action that results in the maximum Q-value among all possible actions from that state. The arg_max operation can make a mistake, especially in the early stages, because the Q-values are still evolving, and we may not have gathered enough information to figure out the best action.\n",
    "\n",
    "Double Q-Learning solves this problem, where we select the best action using one set of parameters w but evaluate it using a different set of parameters w'. It is like having two separate function approximators that must agree on the best action. If they do not, the Q-value returned will be lower. In the long run, this prevents the algorithm from propagating incidental high rewards that may have been obtained by chance and do not reflect long-term returns. When using DQNs with fixed targets, we already have an alternate set of parameters to use as w'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duelling DQN**\n",
    "\n",
    "In the typical DQN architecture, we have a couple of fully connected layers that produce Q-values. In Dueling Networks, we use two streams, one that estimates the state-value function and one that estimates the advantage for each action. These streams may share some layers initially, such as convolutional layers, then branch off with their own fully-connected layers. The desired Q-values are obtained by combining the state and the advantage values. This works because most states' value does not vary across actions, making sense to estimate them directly. The Advantage Function is used to capture the difference actions make in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prioritized Experience Replay**\n",
    "\n",
    "In the Experience Replay buffer, some experiences may be more critical than others; they may also occur infrequently. If we sample the batches uniformly, then these experiences have a minimal chance of getting selected. Since buffers are limited in capacity, older experiences may get lost. We can use the TD error to weight the tuples, such that experiences with a higher error are picked more often from the buffer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
